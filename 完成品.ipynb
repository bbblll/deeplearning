{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要修改成的数据大小\n",
    "dsize = 128\n",
    "# 最大学习率(优化器)\n",
    "max_lr = 0.012 \n",
    "# 正则项权值的衰减(优化器)\n",
    "weight_decay = 1e-4 \n",
    "# 一般0.9 (优化器)\n",
    "momentum = 0.9 \n",
    "# 最小学习率(退火学习)\n",
    "min_lr = 0.001\n",
    "# 设置GPU运行\n",
    "device = torch.device('cuda')\n",
    "# 退火学习，下降次数\n",
    "scheduler_step = 300\n",
    "# 打包个数\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = './competition_data'\n",
    "save_weight_path = src + '/weight'\n",
    "train_image_dir = src + '/train/images'\n",
    "train_mask_dir = src + '/train/masks'\n",
    "test_image_dir = src + '/test/images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取数据id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = pd.read_csv(src + '/train.csv')\n",
    "fold = (list(range(5))*1000)[:len(depths)] # [0,1,2,3,4,0,1,2...]\n",
    "depths['fold'] = fold # 将数据标记为五份\n",
    "all_ids = depths['id'].values # 取出所有id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图片id分为五类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = []\n",
    "for i in range(5):\n",
    "  tem = depths.loc[depths['fold']==i,'id'].values\n",
    "  fold.append(tem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取图片（输入，输出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_images(ids):\n",
    "  images = []\n",
    "  masks = []\n",
    "  for id in ids:\n",
    "    image = plt.imread(train_image_dir+'/'+id+'.png')[0] / 255\n",
    "    mask = plt.imread(train_mask_dir+'/'+id+'.png')[0] / 255\n",
    "    masks.append(mask)\n",
    "    images.append(image)\n",
    "  return images,masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建数据集类型\n",
    "数据集类型有三个常用魔法方法\n",
    "1. 初始化（获取参数）\n",
    "2. 获取数据（数据处理，返回数据）\n",
    "3. 获取数据集长度（返回数据集长度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练数据集\n",
    "class TensorDataset(Dataset):\n",
    "  def __init__(self, data, target):\n",
    "    self.data = data\n",
    "    self.target = target\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # 改变尺寸，并且变为张量\n",
    "    resolved_data = torch.Tensor(\n",
    "      cv2.resize(self.data[index], dsize=(dsize,dsize))\n",
    "    ).reshape(1,dsize,dsize)\n",
    "    # 改变尺寸，并且变为张量\n",
    "    resolved_target = torch.Tensor(\n",
    "      cv2.resize(self.target[index], dsize=(dsize,dsize))\n",
    "    ).reshape(1,dsize,dsize)\n",
    "    # 返回\n",
    "    # (1,128,128),(1,128,128)\n",
    "    return resolved_data,resolved_target\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 我的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self,in_size,out_size) -> None:\n",
    "    super().__init__()\n",
    "    self.layer1 = nn.ConvTranspose2d(in_channels=in_size,out_channels=out_size,kernel_size=2,stride=2,padding=0)\n",
    "    self.layer2 = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=in_size,out_channels=out_size,stride=1,padding=1,kernel_size=3),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "  def forward(self,x1,x2):\n",
    "    y1 = self.layer1(x1)\n",
    "    tem = torch.cat((y1,x2),dim=1)\n",
    "    y2 = self.layer2(tem)\n",
    "    return y2\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "    self.layer1 = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=1,out_channels=4,stride=2,padding=1,kernel_size=3),\n",
    "      nn.BatchNorm2d(num_features=4),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "    self.layer2 = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=4,out_channels=8,stride=2,padding=1,kernel_size=3),\n",
    "      nn.BatchNorm2d(num_features=8),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "    self.layer3 = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=8,out_channels=16,stride=2,padding=1,kernel_size=3),\n",
    "      nn.BatchNorm2d(num_features=16),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "    self.layer4 = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=16,out_channels=32,stride=2,padding=1,kernel_size=3),\n",
    "      nn.BatchNorm2d(num_features=32),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "    self.decoder1 = Decoder(32,16)\n",
    "    self.decoder2 = Decoder(16,8)\n",
    "    self.decoder3 = Decoder(8,4)\n",
    "    self.last = nn.Sequential(\n",
    "      nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "      nn.Conv2d(in_channels=4,kernel_size=5,out_channels=2,padding=2),\n",
    "      nn.Conv2d(in_channels=2,kernel_size=1,out_channels=1)\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "  def forward(self,input):\n",
    "    x1 = self.layer1(input) # torch.Size([18, 4, 64, 64])\n",
    "    x2 = self.layer2(x1) # torch.Size([18, 8, 32, 32])\n",
    "    x3 = self.layer3(x2) # torch.Size([18, 16, 16, 16])\n",
    "    x4 = self.layer4(x3) # torch.Size([18, 32, 8, 8])\n",
    "\n",
    "    y1 = self.decoder1(x4,x3) # torch.Size([18, 16, 16, 16])\n",
    "    y2 = self.decoder2(y1,x2)# torch.Size([18, 8, 32, 32])\n",
    "    y3 = self.decoder3(y2,x1) # torch.Size([18, 4, 64, 64])\n",
    "    output = self.last(y3) # torch.Size([18, 1, 128, 128])\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(key):\n",
    "  model = SimpleUNet()\n",
    "  model.load_state_dict(torch.load(\"./weight/\" + key+ \".pth\"))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建模型对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleUNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (decoder1): Decoder(\n",
       "    (layer1): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (layer2): Sequential(\n",
       "      (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (decoder2): Decoder(\n",
       "    (layer1): ConvTranspose2d(16, 8, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (layer2): Sequential(\n",
       "      (0): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (decoder3): Decoder(\n",
       "    (layer1): ConvTranspose2d(8, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (layer2): Sequential(\n",
       "      (0): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (last): Sequential(\n",
       "    (0): ConvTranspose2d(4, 1, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4))\n",
       "    (2): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salt = SimpleUNet()\n",
    "# salt = get_model(\"0_4\")\n",
    "\n",
    "# GPU 运算\n",
    "salt.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行一次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.SGD(salt.parameters(), lr=max_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, scheduler_step, min_lr)\n",
    "\n",
    "def train(loader_data,model):\n",
    "  running_loss = 0\n",
    "  model.train()\n",
    "  for input,mask in loader_data:\n",
    "    input, mask = input.to(device), mask.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()# 梯度初始化为零\n",
    "    # 使用with，会自动关闭梯度计算\n",
    "    # 设置梯度可算\n",
    "    with torch.set_grad_enabled(True):\n",
    "      logit = model(input)# 进行一次计算\n",
    "      loss = nn.BCEWithLogitsLoss()(logit.squeeze(),mask.squeeze())# 计算误差\n",
    "      loss.backward()# 反馈\n",
    "      optimizer.step()# 进行一次参数更新\n",
    "    running_loss += loss.item()*input.size()[0]# 累计平均误差\n",
    "  epoch_loss = running_loss / len(loader_data)# 计算平均误差\n",
    "  return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行一次测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader_test,model):\n",
    "  running_loss = 0.0\n",
    "  data_size = len(loader_test)\n",
    "  # 测试\n",
    "  model.eval()\n",
    "  for input, mask in loader_test:\n",
    "    input, mask = input.to(device), mask.to(device)\n",
    "    with torch.set_grad_enabled(False):\n",
    "      output = model(input)\n",
    "      loss = nn.BCEWithLogitsLoss()(output.squeeze(), mask.squeeze())\n",
    "    running_loss += loss.item() * input.size(0)\n",
    "  return running_loss/data_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主函数部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train_loss: 3.5850 val_loss: 3.4677  lr: 0.0120\n",
      "epoch: 2 train_loss: 3.5853 val_loss: 3.4670  lr: 0.0120\n",
      "epoch: 3 train_loss: 3.5856 val_loss: 3.4663  lr: 0.0120\n",
      "epoch: 4 train_loss: 3.5855 val_loss: 3.4670  lr: 0.0120\n",
      "epoch: 5 train_loss: 3.5855 val_loss: 3.4693  lr: 0.0120\n",
      "epoch: 6 train_loss: 3.5851 val_loss: 3.4706  lr: 0.0120\n",
      "epoch: 7 train_loss: 3.5857 val_loss: 3.4663  lr: 0.0120\n",
      "epoch: 8 train_loss: 3.5856 val_loss: 3.4689  lr: 0.0120\n",
      "epoch: 9 train_loss: 3.5850 val_loss: 3.4689  lr: 0.0120\n",
      "epoch: 10 train_loss: 3.5855 val_loss: 3.4662  lr: 0.0120\n",
      "epoch: 11 train_loss: 3.5854 val_loss: 3.4706  lr: 0.0120\n",
      "epoch: 12 train_loss: 3.5856 val_loss: 3.4671  lr: 0.0120\n",
      "epoch: 13 train_loss: 3.5849 val_loss: 3.4661  lr: 0.0119\n",
      "epoch: 14 train_loss: 3.5851 val_loss: 3.4675  lr: 0.0119\n",
      "epoch: 15 train_loss: 3.5849 val_loss: 3.4713  lr: 0.0119\n",
      "epoch: 16 train_loss: 3.5850 val_loss: 3.4685  lr: 0.0119\n",
      "epoch: 17 train_loss: 3.5857 val_loss: 3.4665  lr: 0.0119\n",
      "epoch: 18 train_loss: 3.5851 val_loss: 3.4662  lr: 0.0119\n",
      "epoch: 19 train_loss: 3.5852 val_loss: 3.4667  lr: 0.0119\n",
      "epoch: 20 train_loss: 3.5850 val_loss: 3.4661  lr: 0.0119\n",
      "epoch: 21 train_loss: 3.5851 val_loss: 3.4663  lr: 0.0119\n",
      "epoch: 22 train_loss: 3.5854 val_loss: 3.4668  lr: 0.0119\n",
      "epoch: 23 train_loss: 3.5849 val_loss: 3.4668  lr: 0.0118\n",
      "epoch: 24 train_loss: 3.5850 val_loss: 3.4692  lr: 0.0118\n",
      "epoch: 25 train_loss: 3.5853 val_loss: 3.4661  lr: 0.0118\n",
      "epoch: 26 train_loss: 3.5843 val_loss: 3.4661  lr: 0.0118\n",
      "epoch: 27 train_loss: 3.5849 val_loss: 3.4667  lr: 0.0118\n",
      "epoch: 28 train_loss: 3.5849 val_loss: 3.4691  lr: 0.0118\n",
      "epoch: 29 train_loss: 3.5857 val_loss: 3.4666  lr: 0.0117\n",
      "epoch: 30 train_loss: 3.5848 val_loss: 3.4664  lr: 0.0117\n",
      "epoch: 31 train_loss: 3.5850 val_loss: 3.4665  lr: 0.0117\n",
      "epoch: 32 train_loss: 3.5854 val_loss: 3.4663  lr: 0.0117\n",
      "epoch: 33 train_loss: 3.5842 val_loss: 3.4676  lr: 0.0117\n",
      "epoch: 34 train_loss: 3.5850 val_loss: 3.4664  lr: 0.0117\n",
      "epoch: 35 train_loss: 3.5846 val_loss: 3.4690  lr: 0.0116\n",
      "epoch: 36 train_loss: 3.5854 val_loss: 3.4661  lr: 0.0116\n",
      "epoch: 37 train_loss: 3.5846 val_loss: 3.4706  lr: 0.0116\n",
      "epoch: 38 train_loss: 3.5849 val_loss: 3.4693  lr: 0.0116\n",
      "epoch: 39 train_loss: 3.5848 val_loss: 3.4669  lr: 0.0115\n",
      "epoch: 40 train_loss: 3.5847 val_loss: 3.4664  lr: 0.0115\n",
      "epoch: 41 train_loss: 3.5846 val_loss: 3.4660  lr: 0.0115\n",
      "epoch: 42 train_loss: 3.5850 val_loss: 3.4660  lr: 0.0115\n",
      "epoch: 43 train_loss: 3.5848 val_loss: 3.4664  lr: 0.0115\n",
      "epoch: 44 train_loss: 3.5847 val_loss: 3.4671  lr: 0.0114\n",
      "epoch: 45 train_loss: 3.5848 val_loss: 3.4661  lr: 0.0114\n",
      "epoch: 46 train_loss: 3.5849 val_loss: 3.4688  lr: 0.0114\n",
      "epoch: 47 train_loss: 3.5847 val_loss: 3.4692  lr: 0.0113\n",
      "epoch: 48 train_loss: 3.5848 val_loss: 3.4682  lr: 0.0113\n",
      "epoch: 49 train_loss: 3.5845 val_loss: 3.4661  lr: 0.0113\n",
      "epoch: 50 train_loss: 3.5850 val_loss: 3.4663  lr: 0.0113\n",
      "epoch: 51 train_loss: 3.5843 val_loss: 3.4672  lr: 0.0112\n",
      "epoch: 52 train_loss: 3.5848 val_loss: 3.4684  lr: 0.0112\n",
      "epoch: 53 train_loss: 3.5849 val_loss: 3.4663  lr: 0.0112\n",
      "epoch: 54 train_loss: 3.5846 val_loss: 3.4670  lr: 0.0111\n",
      "epoch: 55 train_loss: 3.5848 val_loss: 3.4659  lr: 0.0111\n",
      "epoch: 56 train_loss: 3.5849 val_loss: 3.4666  lr: 0.0111\n",
      "epoch: 57 train_loss: 3.5853 val_loss: 3.4669  lr: 0.0110\n",
      "epoch: 58 train_loss: 3.5847 val_loss: 3.4659  lr: 0.0110\n",
      "epoch: 59 train_loss: 3.5845 val_loss: 3.4671  lr: 0.0110\n",
      "epoch: 60 train_loss: 3.5846 val_loss: 3.4675  lr: 0.0109\n",
      "epoch: 61 train_loss: 3.5851 val_loss: 3.4668  lr: 0.0109\n",
      "epoch: 62 train_loss: 3.5847 val_loss: 3.4674  lr: 0.0109\n",
      "epoch: 63 train_loss: 3.5842 val_loss: 3.4681  lr: 0.0108\n",
      "epoch: 64 train_loss: 3.5850 val_loss: 3.4671  lr: 0.0108\n",
      "epoch: 65 train_loss: 3.5846 val_loss: 3.4721  lr: 0.0108\n",
      "epoch: 66 train_loss: 3.5849 val_loss: 3.4684  lr: 0.0107\n",
      "epoch: 67 train_loss: 3.5853 val_loss: 3.4661  lr: 0.0107\n",
      "epoch: 68 train_loss: 3.5847 val_loss: 3.4659  lr: 0.0107\n",
      "epoch: 69 train_loss: 3.5845 val_loss: 3.4662  lr: 0.0106\n",
      "epoch: 70 train_loss: 3.5852 val_loss: 3.4663  lr: 0.0106\n",
      "epoch: 71 train_loss: 3.5844 val_loss: 3.4658  lr: 0.0105\n",
      "epoch: 72 train_loss: 3.5836 val_loss: 3.4659  lr: 0.0105\n",
      "epoch: 73 train_loss: 3.5846 val_loss: 3.4659  lr: 0.0105\n",
      "epoch: 74 train_loss: 3.5853 val_loss: 3.4673  lr: 0.0104\n",
      "epoch: 75 train_loss: 3.5851 val_loss: 3.4685  lr: 0.0104\n",
      "epoch: 76 train_loss: 3.5835 val_loss: 3.4753  lr: 0.0103\n",
      "epoch: 77 train_loss: 3.5848 val_loss: 3.4658  lr: 0.0103\n",
      "epoch: 78 train_loss: 3.5851 val_loss: 3.4674  lr: 0.0103\n",
      "epoch: 79 train_loss: 3.5845 val_loss: 3.4658  lr: 0.0102\n",
      "epoch: 80 train_loss: 3.5846 val_loss: 3.4658  lr: 0.0102\n",
      "epoch: 81 train_loss: 3.5844 val_loss: 3.4658  lr: 0.0101\n",
      "epoch: 82 train_loss: 3.5844 val_loss: 3.4660  lr: 0.0101\n",
      "epoch: 83 train_loss: 3.5849 val_loss: 3.4694  lr: 0.0101\n",
      "epoch: 84 train_loss: 3.5846 val_loss: 3.4667  lr: 0.0100\n",
      "epoch: 85 train_loss: 3.5846 val_loss: 3.4661  lr: 0.0100\n",
      "epoch: 86 train_loss: 3.5848 val_loss: 3.4658  lr: 0.0099\n",
      "epoch: 87 train_loss: 3.5845 val_loss: 3.4660  lr: 0.0099\n",
      "epoch: 88 train_loss: 3.5839 val_loss: 3.4658  lr: 0.0098\n",
      "epoch: 89 train_loss: 3.5847 val_loss: 3.4671  lr: 0.0098\n",
      "epoch: 90 train_loss: 3.5836 val_loss: 3.4665  lr: 0.0097\n",
      "epoch: 91 train_loss: 3.5844 val_loss: 3.4658  lr: 0.0097\n",
      "epoch: 92 train_loss: 3.5847 val_loss: 3.4658  lr: 0.0096\n",
      "epoch: 93 train_loss: 3.5847 val_loss: 3.4670  lr: 0.0096\n",
      "epoch: 94 train_loss: 3.5848 val_loss: 3.4660  lr: 0.0095\n",
      "epoch: 95 train_loss: 3.5844 val_loss: 3.4674  lr: 0.0095\n",
      "epoch: 96 train_loss: 3.5845 val_loss: 3.4683  lr: 0.0094\n",
      "epoch: 97 train_loss: 3.5845 val_loss: 3.4672  lr: 0.0094\n",
      "epoch: 98 train_loss: 3.5846 val_loss: 3.4666  lr: 0.0093\n",
      "epoch: 99 train_loss: 3.5845 val_loss: 3.4681  lr: 0.0093\n",
      "epoch: 100 train_loss: 3.5844 val_loss: 3.4666  lr: 0.0092\n",
      "epoch: 101 train_loss: 3.5843 val_loss: 3.4682  lr: 0.0092\n",
      "epoch: 102 train_loss: 3.5843 val_loss: 3.4705  lr: 0.0091\n",
      "epoch: 103 train_loss: 3.5850 val_loss: 3.4675  lr: 0.0091\n",
      "epoch: 104 train_loss: 3.5843 val_loss: 3.4660  lr: 0.0090\n",
      "epoch: 105 train_loss: 3.5845 val_loss: 3.4684  lr: 0.0090\n",
      "epoch: 106 train_loss: 3.5843 val_loss: 3.4673  lr: 0.0089\n",
      "epoch: 107 train_loss: 3.5847 val_loss: 3.4662  lr: 0.0089\n",
      "epoch: 108 train_loss: 3.5839 val_loss: 3.4657  lr: 0.0088\n",
      "epoch: 109 train_loss: 3.5842 val_loss: 3.4671  lr: 0.0088\n",
      "epoch: 110 train_loss: 3.5848 val_loss: 3.4664  lr: 0.0087\n",
      "epoch: 111 train_loss: 3.5837 val_loss: 3.4657  lr: 0.0087\n",
      "epoch: 112 train_loss: 3.5845 val_loss: 3.4659  lr: 0.0086\n",
      "epoch: 113 train_loss: 3.5845 val_loss: 3.4667  lr: 0.0086\n",
      "epoch: 114 train_loss: 3.5846 val_loss: 3.4659  lr: 0.0085\n",
      "epoch: 115 train_loss: 3.5843 val_loss: 3.4658  lr: 0.0085\n",
      "epoch: 116 train_loss: 3.5849 val_loss: 3.4671  lr: 0.0084\n",
      "epoch: 117 train_loss: 3.5844 val_loss: 3.4657  lr: 0.0084\n",
      "epoch: 118 train_loss: 3.5846 val_loss: 3.4659  lr: 0.0083\n",
      "epoch: 119 train_loss: 3.5842 val_loss: 3.4659  lr: 0.0083\n",
      "epoch: 120 train_loss: 3.5845 val_loss: 3.4664  lr: 0.0082\n",
      "epoch: 121 train_loss: 3.5842 val_loss: 3.4661  lr: 0.0081\n",
      "epoch: 122 train_loss: 3.5843 val_loss: 3.4668  lr: 0.0081\n",
      "epoch: 123 train_loss: 3.5843 val_loss: 3.4662  lr: 0.0080\n",
      "epoch: 124 train_loss: 3.5842 val_loss: 3.4657  lr: 0.0080\n",
      "epoch: 125 train_loss: 3.5841 val_loss: 3.4679  lr: 0.0079\n",
      "epoch: 126 train_loss: 3.5842 val_loss: 3.4666  lr: 0.0079\n",
      "epoch: 127 train_loss: 3.5843 val_loss: 3.4657  lr: 0.0078\n",
      "epoch: 128 train_loss: 3.5843 val_loss: 3.4666  lr: 0.0078\n",
      "epoch: 129 train_loss: 3.5843 val_loss: 3.4661  lr: 0.0077\n",
      "epoch: 130 train_loss: 3.5843 val_loss: 3.4658  lr: 0.0076\n",
      "epoch: 131 train_loss: 3.5843 val_loss: 3.4660  lr: 0.0076\n",
      "epoch: 132 train_loss: 3.5844 val_loss: 3.4679  lr: 0.0075\n",
      "epoch: 133 train_loss: 3.5843 val_loss: 3.4668  lr: 0.0075\n",
      "epoch: 134 train_loss: 3.5844 val_loss: 3.4667  lr: 0.0074\n",
      "epoch: 135 train_loss: 3.5840 val_loss: 3.4671  lr: 0.0074\n",
      "epoch: 136 train_loss: 3.5837 val_loss: 3.4684  lr: 0.0073\n",
      "epoch: 137 train_loss: 3.5841 val_loss: 3.4663  lr: 0.0072\n",
      "epoch: 138 train_loss: 3.5842 val_loss: 3.4675  lr: 0.0072\n",
      "epoch: 139 train_loss: 3.5844 val_loss: 3.4657  lr: 0.0071\n",
      "epoch: 140 train_loss: 3.5841 val_loss: 3.4657  lr: 0.0071\n",
      "epoch: 141 train_loss: 3.5842 val_loss: 3.4660  lr: 0.0070\n",
      "epoch: 142 train_loss: 3.5843 val_loss: 3.4667  lr: 0.0070\n",
      "epoch: 143 train_loss: 3.5842 val_loss: 3.4661  lr: 0.0069\n",
      "epoch: 144 train_loss: 3.5838 val_loss: 3.4661  lr: 0.0068\n",
      "epoch: 145 train_loss: 3.5842 val_loss: 3.4663  lr: 0.0068\n",
      "epoch: 146 train_loss: 3.5834 val_loss: 3.4692  lr: 0.0067\n",
      "epoch: 147 train_loss: 3.5845 val_loss: 3.4662  lr: 0.0067\n",
      "epoch: 148 train_loss: 3.5840 val_loss: 3.4659  lr: 0.0066\n",
      "epoch: 149 train_loss: 3.5836 val_loss: 3.4657  lr: 0.0066\n",
      "epoch: 150 train_loss: 3.5842 val_loss: 3.4656  lr: 0.0065\n",
      "epoch: 151 train_loss: 3.5843 val_loss: 3.4665  lr: 0.0064\n",
      "epoch: 152 train_loss: 3.5836 val_loss: 3.4680  lr: 0.0064\n",
      "epoch: 153 train_loss: 3.5839 val_loss: 3.4679  lr: 0.0063\n",
      "epoch: 154 train_loss: 3.5845 val_loss: 3.4663  lr: 0.0063\n",
      "epoch: 155 train_loss: 3.5840 val_loss: 3.4660  lr: 0.0062\n",
      "epoch: 156 train_loss: 3.5842 val_loss: 3.4663  lr: 0.0062\n",
      "epoch: 157 train_loss: 3.5841 val_loss: 3.4683  lr: 0.0061\n",
      "epoch: 158 train_loss: 3.5844 val_loss: 3.4658  lr: 0.0060\n",
      "epoch: 159 train_loss: 3.5838 val_loss: 3.4657  lr: 0.0060\n",
      "epoch: 160 train_loss: 3.5839 val_loss: 3.4658  lr: 0.0059\n",
      "epoch: 161 train_loss: 3.5836 val_loss: 3.4658  lr: 0.0059\n",
      "epoch: 162 train_loss: 3.5843 val_loss: 3.4657  lr: 0.0058\n",
      "epoch: 163 train_loss: 3.5841 val_loss: 3.4660  lr: 0.0058\n",
      "epoch: 164 train_loss: 3.5837 val_loss: 3.4657  lr: 0.0057\n",
      "epoch: 165 train_loss: 3.5843 val_loss: 3.4658  lr: 0.0056\n",
      "epoch: 166 train_loss: 3.5841 val_loss: 3.4668  lr: 0.0056\n",
      "epoch: 167 train_loss: 3.5840 val_loss: 3.4657  lr: 0.0055\n",
      "epoch: 168 train_loss: 3.5839 val_loss: 3.4662  lr: 0.0055\n",
      "epoch: 169 train_loss: 3.5838 val_loss: 3.4661  lr: 0.0054\n",
      "epoch: 170 train_loss: 3.5839 val_loss: 3.4665  lr: 0.0054\n",
      "epoch: 171 train_loss: 3.5841 val_loss: 3.4667  lr: 0.0053\n",
      "epoch: 172 train_loss: 3.5839 val_loss: 3.4664  lr: 0.0052\n",
      "epoch: 173 train_loss: 3.5841 val_loss: 3.4659  lr: 0.0052\n",
      "epoch: 174 train_loss: 3.5838 val_loss: 3.4667  lr: 0.0051\n",
      "epoch: 175 train_loss: 3.5841 val_loss: 3.4660  lr: 0.0051\n",
      "epoch: 176 train_loss: 3.5840 val_loss: 3.4663  lr: 0.0050\n",
      "epoch: 177 train_loss: 3.5841 val_loss: 3.4660  lr: 0.0050\n",
      "epoch: 178 train_loss: 3.5838 val_loss: 3.4659  lr: 0.0049\n",
      "epoch: 179 train_loss: 3.5840 val_loss: 3.4662  lr: 0.0049\n",
      "epoch: 180 train_loss: 3.5841 val_loss: 3.4658  lr: 0.0048\n",
      "epoch: 181 train_loss: 3.5839 val_loss: 3.4661  lr: 0.0047\n",
      "epoch: 182 train_loss: 3.5842 val_loss: 3.4666  lr: 0.0047\n",
      "epoch: 183 train_loss: 3.5838 val_loss: 3.4672  lr: 0.0046\n",
      "epoch: 184 train_loss: 3.5839 val_loss: 3.4662  lr: 0.0046\n",
      "epoch: 185 train_loss: 3.5839 val_loss: 3.4662  lr: 0.0045\n",
      "epoch: 186 train_loss: 3.5839 val_loss: 3.4667  lr: 0.0045\n",
      "epoch: 187 train_loss: 3.5840 val_loss: 3.4661  lr: 0.0044\n",
      "epoch: 188 train_loss: 3.5841 val_loss: 3.4664  lr: 0.0044\n",
      "epoch: 189 train_loss: 3.5839 val_loss: 3.4666  lr: 0.0043\n",
      "epoch: 190 train_loss: 3.5838 val_loss: 3.4662  lr: 0.0043\n",
      "epoch: 191 train_loss: 3.5838 val_loss: 3.4666  lr: 0.0042\n",
      "epoch: 192 train_loss: 3.5836 val_loss: 3.4671  lr: 0.0042\n",
      "epoch: 193 train_loss: 3.5840 val_loss: 3.4663  lr: 0.0041\n",
      "epoch: 194 train_loss: 3.5836 val_loss: 3.4672  lr: 0.0041\n",
      "epoch: 195 train_loss: 3.5840 val_loss: 3.4666  lr: 0.0040\n",
      "epoch: 196 train_loss: 3.5838 val_loss: 3.4661  lr: 0.0040\n",
      "epoch: 197 train_loss: 3.5837 val_loss: 3.4676  lr: 0.0039\n",
      "epoch: 198 train_loss: 3.5840 val_loss: 3.4658  lr: 0.0039\n",
      "epoch: 199 train_loss: 3.5838 val_loss: 3.4659  lr: 0.0038\n",
      "epoch: 200 train_loss: 3.5839 val_loss: 3.4658  lr: 0.0037\n",
      "epoch: 201 train_loss: 3.5840 val_loss: 3.4657  lr: 0.0037\n",
      "epoch: 202 train_loss: 3.5839 val_loss: 3.4660  lr: 0.0037\n",
      "epoch: 203 train_loss: 3.5838 val_loss: 3.4662  lr: 0.0036\n",
      "epoch: 204 train_loss: 3.5837 val_loss: 3.4656  lr: 0.0036\n",
      "epoch: 205 train_loss: 3.5839 val_loss: 3.4657  lr: 0.0035\n",
      "epoch: 206 train_loss: 3.5837 val_loss: 3.4669  lr: 0.0035\n",
      "epoch: 207 train_loss: 3.5839 val_loss: 3.4658  lr: 0.0034\n",
      "epoch: 208 train_loss: 3.5836 val_loss: 3.4656  lr: 0.0034\n",
      "epoch: 209 train_loss: 3.5841 val_loss: 3.4661  lr: 0.0033\n",
      "epoch: 210 train_loss: 3.5836 val_loss: 3.4671  lr: 0.0033\n",
      "epoch: 211 train_loss: 3.5839 val_loss: 3.4664  lr: 0.0032\n",
      "epoch: 212 train_loss: 3.5837 val_loss: 3.4673  lr: 0.0032\n",
      "epoch: 213 train_loss: 3.5841 val_loss: 3.4665  lr: 0.0031\n",
      "epoch: 214 train_loss: 3.5837 val_loss: 3.4667  lr: 0.0031\n",
      "epoch: 215 train_loss: 3.5838 val_loss: 3.4657  lr: 0.0030\n",
      "epoch: 216 train_loss: 3.5839 val_loss: 3.4660  lr: 0.0030\n",
      "epoch: 217 train_loss: 3.5839 val_loss: 3.4664  lr: 0.0029\n",
      "epoch: 218 train_loss: 3.5838 val_loss: 3.4661  lr: 0.0029\n",
      "epoch: 219 train_loss: 3.5838 val_loss: 3.4657  lr: 0.0029\n",
      "epoch: 220 train_loss: 3.5838 val_loss: 3.4662  lr: 0.0028\n",
      "epoch: 221 train_loss: 3.5837 val_loss: 3.4663  lr: 0.0028\n",
      "epoch: 222 train_loss: 3.5838 val_loss: 3.4662  lr: 0.0027\n",
      "epoch: 223 train_loss: 3.5838 val_loss: 3.4661  lr: 0.0027\n",
      "epoch: 224 train_loss: 3.5837 val_loss: 3.4661  lr: 0.0027\n",
      "epoch: 225 train_loss: 3.5837 val_loss: 3.4660  lr: 0.0026\n",
      "epoch: 226 train_loss: 3.5838 val_loss: 3.4658  lr: 0.0026\n",
      "epoch: 227 train_loss: 3.5837 val_loss: 3.4659  lr: 0.0025\n",
      "epoch: 228 train_loss: 3.5837 val_loss: 3.4660  lr: 0.0025\n",
      "epoch: 229 train_loss: 3.5837 val_loss: 3.4659  lr: 0.0025\n",
      "epoch: 230 train_loss: 3.5838 val_loss: 3.4660  lr: 0.0024\n",
      "epoch: 231 train_loss: 3.5836 val_loss: 3.4666  lr: 0.0024\n",
      "epoch: 232 train_loss: 3.5838 val_loss: 3.4661  lr: 0.0023\n",
      "epoch: 233 train_loss: 3.5837 val_loss: 3.4663  lr: 0.0023\n",
      "epoch: 234 train_loss: 3.5837 val_loss: 3.4660  lr: 0.0023\n",
      "epoch: 235 train_loss: 3.5835 val_loss: 3.4666  lr: 0.0022\n",
      "epoch: 236 train_loss: 3.5838 val_loss: 3.4664  lr: 0.0022\n",
      "epoch: 237 train_loss: 3.5837 val_loss: 3.4663  lr: 0.0022\n",
      "epoch: 238 train_loss: 3.5837 val_loss: 3.4660  lr: 0.0021\n",
      "epoch: 239 train_loss: 3.5837 val_loss: 3.4660  lr: 0.0021\n",
      "epoch: 240 train_loss: 3.5836 val_loss: 3.4665  lr: 0.0021\n",
      "epoch: 241 train_loss: 3.5838 val_loss: 3.4663  lr: 0.0020\n",
      "epoch: 242 train_loss: 3.5838 val_loss: 3.4660  lr: 0.0020\n",
      "epoch: 243 train_loss: 3.5837 val_loss: 3.4664  lr: 0.0020\n",
      "epoch: 244 train_loss: 3.5838 val_loss: 3.4662  lr: 0.0019\n",
      "epoch: 245 train_loss: 3.5837 val_loss: 3.4662  lr: 0.0019\n",
      "epoch: 246 train_loss: 3.5836 val_loss: 3.4663  lr: 0.0019\n",
      "epoch: 247 train_loss: 3.5836 val_loss: 3.4659  lr: 0.0018\n",
      "epoch: 248 train_loss: 3.5836 val_loss: 3.4659  lr: 0.0018\n",
      "epoch: 249 train_loss: 3.5836 val_loss: 3.4662  lr: 0.0018\n",
      "epoch: 250 train_loss: 3.5837 val_loss: 3.4660  lr: 0.0017\n",
      "epoch: 251 train_loss: 3.5837 val_loss: 3.4660  lr: 0.0017\n",
      "epoch: 252 train_loss: 3.5837 val_loss: 3.4660  lr: 0.0017\n"
     ]
    }
   ],
   "source": [
    "for idx in range(5):\n",
    "\n",
    "  optimizer = torch.optim.SGD(salt.parameters(), lr=max_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "  lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, scheduler_step, min_lr)\n",
    "\n",
    "  # setdiff1d 取不同的元素\n",
    "  train_id = np.setdiff1d(all_ids, fold[idx])\n",
    "  val_id = fold[idx]\n",
    "  # 取出数据\n",
    "  X_train, y_train = get_train_images(train_id)\n",
    "  X_val, y_val = get_train_images(val_id)\n",
    "  # 制作数据集\n",
    "  train_data = TensorDataset(X_train, y_train)\n",
    "  val_data = TensorDataset(X_val, y_val)\n",
    "  # 打乱，制作可迭代数据集\n",
    "  train_loader = DataLoader(train_data,shuffle=True,batch_size=batch_size) \n",
    "  val_loader = DataLoader(val_data,shuffle=False,batch_size=batch_size) \n",
    "\n",
    "  num_snapshot = 0\n",
    "  lowest_loss = 10000\n",
    "# 训练\n",
    "  for epoch_ in range(300): # 300\n",
    "    train_loss = train(train_loader, salt)\n",
    "    val_loss = test(val_loader, salt)\n",
    "    # 每训练一次调整学习率（退火学习）\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    if lowest_loss > val_loss:\n",
    "      lowest_loss = val_loss\n",
    "      best_param = salt.state_dict()\n",
    "\n",
    "    # 调节一个\n",
    "    if (epoch_ + 1) % scheduler_step == 0:\n",
    "      torch.save(best_param, \"./weight/\" + str(idx) +\"_\"+ str(num_snapshot) + '.pth')\n",
    "      # 重置优化器，以及退火学习\n",
    "      optimizer = torch.optim.SGD(salt.parameters(), lr=max_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "      lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, scheduler_step, min_lr)\n",
    "      num_snapshot += 1\n",
    "      lowest_loss = 10000\n",
    "\n",
    "    print('epoch: {} train_loss: {:.4f} val_loss: {:.4f}  lr: {:.4f}'.format(epoch_ + 1, train_loss*100, val_loss*100, lr_scheduler.get_last_lr()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch 保存参数\n",
    "\n",
    "|操作|函数|\n",
    "|-|-|\n",
    "|保存|torch.save(model.state_dict(),path)|\n",
    "|读取|torch.load(path)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6e7463ab38ffa65d2678dd98ae9d6c9783a580bfd91baaccc455120c17d4d4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
