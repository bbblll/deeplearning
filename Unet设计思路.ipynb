{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b902261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e3e55b",
   "metadata": {},
   "source": [
    "### GPU运行工具，后面会用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d0973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b247f80c",
   "metadata": {},
   "source": [
    "### 创建根目录，利用跟目录找到其他目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "535d98a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = './competition_data'\n",
    "save_weight_path = src + '/weight'\n",
    "train_image_dir = src + '/train/images'\n",
    "train_mask_dir = src + '/train/masks'\n",
    "test_image_dir = src + '/test/images'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac086f66",
   "metadata": {},
   "source": [
    "1. 读取depth数据\n",
    "2. 按z的大小排序，替换\n",
    "3. 删除z列，替换\n",
    "4. 分组\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c5d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = pd.read_csv(src + '/depths.csv')\n",
    "depths.sort_values('z', inplace=True)\n",
    "depths.drop('z', axis=1, inplace=True)\n",
    "depths['fold'] = (list(range(0,5)) * depths.shape[0])[:depths.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee8485",
   "metadata": {},
   "source": [
    "1. 读取train的数据\n",
    "2. 合并数据\n",
    "3. 创建空数列dist\n",
    "4. 读取图片，灰度图（101，101）\n",
    "5. 计算图片不同像素值个数\n",
    "6. 存入dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b0618d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(src + '/train.csv')\n",
    "train_df = train_df.merge(depths)\n",
    "dist = []\n",
    "\n",
    "for id in train_df.id.values:\n",
    "  # f的使用就是将大括号内的变量转为字符\n",
    "  img = plt.imread(train_image_dir+f'/{id}.png')\n",
    "  dist.append(np.unique(img).shape[0])\n",
    "train_df['unique_pixels'] = dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c09c7",
   "metadata": {},
   "source": [
    "1. 获取id列表\n",
    "2. 根据fold对数据进行分组（5组）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4e79dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_id = train_df['id'].values\n",
    "fold = []\n",
    "for i in range(5):\n",
    "  fold.append(train_df.loc[train_df['fold']==i, 'id'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d6b61",
   "metadata": {},
   "source": [
    "## 扩张\n",
    "|函数|作用|描述|\n",
    "|-|-|-|\n",
    "|ConvTranspose2d|扩张，倍数与stride有关|输入有限制|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95f657d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([64, 64, 32, 32])\n",
      "output torch.Size([64, 32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "ConvTranspose2d = nn.ConvTranspose2d(64,32,2,2)\n",
    "input  = torch.Tensor(64,64,32,32)\n",
    "print('input',input.shape)\n",
    "output = ConvTranspose2d(input)\n",
    "print('output',output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aca6ad",
   "metadata": {},
   "source": [
    "## 归一化\n",
    "|函数|作用|描述|\n",
    "|-|-|-|\n",
    "|BatchNorm2d|归一化，防止梯度消失或则过大|输入有限制，且对输出维度不影响|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a8dd543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([64, 64, 32, 32])\n",
      "output torch.Size([64, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "BatchNorm2d = nn.BatchNorm2d(num_features=64)\n",
    "input  = torch.Tensor(64,64,32,32)\n",
    "print('input',input.shape)\n",
    "output = BatchNorm2d(input)\n",
    "print('output',output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023b9205",
   "metadata": {},
   "source": [
    "## 最大池化\n",
    "|函数|作用|描述|\n",
    "|-|-|-|\n",
    "|BatchNorm2d|最大池化，缩小倍数与stride有关，防止过拟合|输入无限制，|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "205ecd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([64, 64, 32, 32])\n",
      "output torch.Size([64, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "BatchNorm2d = nn.MaxPool2d(kernel_size=2,stride=2,dilation=1)\n",
    "input  = torch.Tensor(64,64,32,32)\n",
    "print('input',input.shape)\n",
    "output = BatchNorm2d(input)\n",
    "print('output',output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd51dfd",
   "metadata": {},
   "source": [
    "## 卷积\n",
    "|函数|作用|描述|\n",
    "|-|-|-|\n",
    "|Conv2d|卷积层，增加数据深度，<br>相比于全连接参数更少，适合更深的模型|输入有限制，<br>参数调节输出深度，以及长度宽度|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ef61f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([64, 1, 32, 32])\n",
      "output torch.Size([64, 8, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "Conv2d= nn.Conv2d(in_channels=1,out_channels=8,kernel_size=7,stride=2,padding=2)\n",
    "input  = torch.Tensor(64,1,32,32)\n",
    "print('input',input.shape)\n",
    "output = Conv2d(input)\n",
    "print('output',output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f34c48",
   "metadata": {},
   "source": [
    "## 全连接\n",
    "|函数|作用|描述|\n",
    "|-|-|-|\n",
    "|Linear|全连接层，所有变量都与参数相连接，<br>拟合能力强，但是运算量大|输入只能是1维，<br>调节输出长度|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1472d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([65536])\n",
      "output torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "Linear= nn.Linear(in_features=64*32*32,out_features=3)\n",
    "input  = torch.Tensor(64,1,32,32).reshape(-1)\n",
    "print('input',input.shape)\n",
    "output = Linear(input)\n",
    "print('output',output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba7ba8",
   "metadata": {},
   "source": [
    "# 激活函数\n",
    "|函数|作用|描述|\n",
    "|-|-|-|\n",
    "|ReLU|激活函数<br>是神经网络能够拟合非线性的关键<br>不改变数据维度|输入无限制<br>输出无限制|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a09a2810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([64, 1, 32, 32])\n",
      "output torch.Size([64, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "ReLU= nn.ReLU()\n",
    "input  = torch.Tensor(64,1,32,32)\n",
    "print('input',input.shape)\n",
    "output = ReLU(input)\n",
    "print('output',output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaca4aa2",
   "metadata": {},
   "source": [
    "# 上采样\n",
    "|函数|作用|描述|\n",
    "|-|-|-|\n",
    "|Upsample|上采样层<br>和转置卷积类似<br>用于扩张|scale_factor决定扩大倍数|\n",
    "|UpsamplingBilinear2d|线性上层采样<br>||\n",
    "|UpsamplingNearest2d|最近上层采样||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bfe6792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([64, 1, 32, 32])\n",
      "output torch.Size([64, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Upsample= nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "input  = torch.Tensor(64,1,32,32)\n",
    "print('input',input.shape)\n",
    "output = Upsample(input)\n",
    "print('output',output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25ea8bf",
   "metadata": {},
   "source": [
    "## UNet网络设计思路\n",
    "#### 1. 我们有两个输出<br>\n",
    "+ output_1 (3,64,64,64)<br>\n",
    "+ output_2 (3,128,32,32)<br>\n",
    "+ 可以看到input_2更加深，但是切面更小，是后面层的输出\n",
    "\n",
    "#### 2. 如何将两个输出一起放入卷积层处理？\n",
    "+ Convtranspose对input_2进行扩张，\n",
    "+ 让其切面大小与input_1一致在进行拼接\n",
    "\n",
    "#### 3. 再对拼接后的数据进行卷积\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e667761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output torch.Size([3, 256, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "from turtle import forward\n",
    "\n",
    "output_1 = torch.Tensor(3,64,64,64)\n",
    "output_2 = torch.Tensor(3,128,32,32)\n",
    "\n",
    "# 定义网络模型\n",
    "class Unet(nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "\n",
    "    self.layer1 = nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=4,stride=2,padding=1)\n",
    "    self.layer2 = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=128,out_channels=256,kernel_size=5,stride=2,padding=2),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "\n",
    "  def forward(self,in_1,in_2):\n",
    "    x1 = self.layer1(in_2) # output torch.Size([3, 64, 64, 64])\n",
    "    x1 = torch.cat((x1,output_1),dim=1) # output torch.Size([3, 128, 64, 64])\n",
    "    x2 = self.layer2(x1) # output torch.Size([3, 256, 32, 32])\n",
    "    return x2\n",
    "\n",
    "my_unet = Unet()\n",
    "output = my_unet(output_1,output_2)\n",
    "print('output',output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "d6e7463ab38ffa65d2678dd98ae9d6c9783a580bfd91baaccc455120c17d4d4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
